# Task ID: 8
# Title: Implement Comprehensive Testing Suite
# Status: pending
# Dependencies: 5, 7
# Priority: medium
# Description: Create a comprehensive testing suite for both Python and Rust implementations, including unit tests, integration tests, and performance benchmarks.
# Details:
1. Implement Python unit tests:
   ```python
   import unittest
   import torch
   from src.model import DualPurposeDistilBERT
   from src.api import DualPurposeClassifier
   
   class TestDualPurposeModel(unittest.TestCase):
       @classmethod
       def setUpClass(cls):
           cls.model = DualPurposeDistilBERT(num_categories=10, num_pii_types=5)
           cls.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
       
       def test_model_architecture(self):
           # Test model structure
           self.assertIsInstance(self.model.distilbert, DistilBertModel)
           self.assertIsInstance(self.model.category_head, nn.Module)
           self.assertIsInstance(self.model.pii_head, nn.Module)
       
       def test_forward_pass(self):
           # Test forward pass
           inputs = self.tokenizer("This is a test", return_tensors='pt')
           outputs = self.model(**inputs)
           
           self.assertIn('category_logits', outputs)
           self.assertIn('pii_logits', outputs)
           
           # Check shapes
           self.assertEqual(outputs['category_logits'].shape, (1, 10))
           self.assertEqual(outputs['pii_logits'].shape[:-1], inputs['input_ids'].shape)
           self.assertEqual(outputs['pii_logits'].shape[-1], 5)
   ```

2. Implement Rust unit tests:
   ```rust
   #[cfg(test)]
   mod tests {
       use super::*;
       use candle_core::{Device, Tensor};
       
       #[test]
       fn test_model_architecture() {
           let device = Device::Cpu;
           let config = DistilBertConfig::default();
           let vb = VarBuilder::zeros(DType::F32, &device);
           
           let model = DualPurposeDistilBERT::new(&config, 10, 5, vb).unwrap();
           
           // Test model structure
           assert!(model.distilbert.embeddings.word_embeddings.weight.dims() == &[config.vocab_size as usize, config.hidden_size as usize]);
       }
       
       #[test]
       fn test_forward_pass() {
           let device = Device::Cpu;
           let config = DistilBertConfig::default();
           let vb = VarBuilder::zeros(DType::F32, &device);
           
           let model = DualPurposeDistilBERT::new(&config, 10, 5, vb).unwrap();
           
           // Create dummy input
           let input_ids = Tensor::new(&[0, 1, 2, 3], &device).unwrap().unsqueeze(0).unwrap();
           let attention_mask = Tensor::new(&[1, 1, 1, 1], &device).unwrap().unsqueeze(0).unwrap();
           
           let (category_logits, pii_logits) = model.forward(&input_ids, Some(&attention_mask)).unwrap();
           
           // Check shapes
           assert_eq!(category_logits.dims(), &[1, 10]);
           assert_eq!(pii_logits.dims(), &[1, 4, 5]);
       }
   }
   ```

3. Implement integration tests:
   ```python
   class TestIntegration(unittest.TestCase):
       @classmethod
       def setUpClass(cls):
           cls.classifier = DualPurposeClassifier(model_path="path/to/model")
       
       def test_end_to_end_classification(self):
           # Test with various inputs
           test_cases = [
               ("This is a normal text about technology.", "technology", 0),
               ("My email is john.doe@example.com and phone is 555-123-4567.", "personal", 2),
               # Add more test cases
           ]
           
           for text, expected_category, expected_pii_count in test_cases:
               result = self.classifier.classify(text)
               self.assertEqual(result['category'], expected_category)
               self.assertEqual(len(result['pii_entities']), expected_pii_count)
   ```

4. Implement performance benchmarks:
   ```python
   import time
   import psutil
   import numpy as np
   
   def benchmark_model(model, tokenizer, texts, batch_sizes=[1, 4, 8, 16], device='cpu'):
       model.to(device)
       model.eval()
       
       results = {}
       
       for batch_size in batch_sizes:
           # Prepare batches
           batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
           
           # Measure inference time
           start_time = time.time()
           memory_usage = []
           
           with torch.no_grad():
               for batch in batches:
                   inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True)
                   inputs = {k: v.to(device) for k, v in inputs.items()}
                   
                   _ = model(**inputs)
                   
                   # Measure memory
                   if device == 'cuda':
                       memory_usage.append(torch.cuda.memory_allocated())
                   else:
                       memory_usage.append(psutil.Process().memory_info().rss)
           
           end_time = time.time()
           total_time = end_time - start_time
           avg_time_per_sample = total_time / len(texts)
           
           results[batch_size] = {
               'total_time': total_time,
               'avg_time_per_sample': avg_time_per_sample,
               'avg_memory': np.mean(memory_usage),
               'max_memory': max(memory_usage)
           }
       
       return results
   ```

5. Implement memory usage monitoring:
   ```python
   def monitor_memory_usage(func):
       def wrapper(*args, **kwargs):
           # Record memory before
           if torch.cuda.is_available():
               torch.cuda.reset_peak_memory_stats()
               before_memory = torch.cuda.memory_allocated()
           else:
               before_memory = psutil.Process().memory_info().rss
           
           # Call function
           result = func(*args, **kwargs)
           
           # Record memory after
           if torch.cuda.is_available():
               after_memory = torch.cuda.max_memory_allocated()
           else:
               after_memory = psutil.Process().memory_info().rss
           
           memory_used = after_memory - before_memory
           print(f"Memory used by {func.__name__}: {memory_used / 1024 / 1024:.2f} MB")
           
           return result
       
       return wrapper
   ```

# Test Strategy:
1. Run all unit tests for both Python and Rust implementations
2. Run integration tests to verify end-to-end functionality
3. Run performance benchmarks with various batch sizes and input lengths
4. Compare memory usage between original and optimized models
5. Verify that performance meets requirements (<100ms per request)
6. Test with edge cases (empty input, very long input, etc.)
7. Test with real-world data samples
