# Task ID: 4
# Title: Implement Model Quantization and Optimization
# Status: pending
# Dependencies: 2
# Priority: medium
# Description: Implement quantization techniques for the dual-purpose model to reduce memory footprint and improve inference speed while maintaining accuracy.
# Details:
1. Implement post-training quantization using PyTorch:
   ```python
   def quantize_model(model, quantization_type='dynamic'):
       if quantization_type == 'dynamic':
           # Dynamic quantization
           quantized_model = torch.quantization.quantize_dynamic(
               model, {torch.nn.Linear}, dtype=torch.qint8
           )
       elif quantization_type == 'static':
           # Static quantization (requires calibration)
           model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
           torch.quantization.prepare(model, inplace=True)
           # Calibration code here
           quantized_model = torch.quantization.convert(model, inplace=False)
       
       return quantized_model
   ```

2. Implement model pruning to reduce parameters:
   ```python
   def prune_model(model, pruning_method='magnitude', amount=0.3):
       for name, module in model.named_modules():
           if isinstance(module, torch.nn.Linear):
               if pruning_method == 'magnitude':
                   prune.l1_unstructured(module, name='weight', amount=amount)
               elif pruning_method == 'structured':
                   prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)
       
       # Make pruning permanent
       for name, module in model.named_modules():
           if isinstance(module, torch.nn.Linear):
               prune.remove(module, 'weight')
       
       return model
   ```

3. Implement knowledge distillation to further optimize the model:
   ```python
   def distill_knowledge(teacher_model, student_model, train_dataloader, optimizer, temperature=2.0):
       # Knowledge distillation implementation
       # [Implementation details]
   ```

4. Implement benchmark function to compare performance:
   ```python
   def benchmark_model(model, test_dataloader, device):
       model.eval()
       start_time = time.time()
       
       with torch.no_grad():
           for batch in test_dataloader:
               input_ids = batch['input_ids'].to(device)
               attention_mask = batch['attention_mask'].to(device)
               _ = model(input_ids=input_ids, attention_mask=attention_mask)
       
       end_time = time.time()
       inference_time = (end_time - start_time) / len(test_dataloader)
       
       return {
           'inference_time': inference_time,
           'memory_usage': torch.cuda.max_memory_allocated(device) if torch.cuda.is_available() else 'N/A'
       }
   ```

# Test Strategy:
1. Compare accuracy before and after quantization to ensure minimal degradation
2. Benchmark inference speed for original vs. quantized models
3. Measure memory usage reduction
4. Test with different quantization methods (dynamic, static, int8, float16)
5. Verify that target inference time (<100ms per request) is achieved
