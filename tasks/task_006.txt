# Task ID: 6
# Title: Port DistilBERT Dual-Purpose Model to Rust using Candle
# Status: pending
# Dependencies: 2, 4
# Priority: high
# Description: Implement the dual-purpose DistilBERT model in Rust using the Candle framework, ensuring compatibility with the Python implementation.
# Details:
1. Define model architecture in Rust:
   ```rust
   use candle_core::{DType, Device, Result, Tensor};
   use candle_nn::{Linear, Module, VarBuilder};
   use candle_transformers::models::distilbert::{DistilBertModel, DistilBertConfig};
   
   struct CategoryClassificationHead {
       dropout: f64,
       classifier: Linear,
   }
   
   impl CategoryClassificationHead {
       fn new(hidden_size: usize, num_categories: usize, vb: VarBuilder) -> Result<Self> {
           let classifier = candle_nn::linear(hidden_size, num_categories, vb.pp("classifier"))?;
           Ok(Self { dropout: 0.1, classifier })
       }
   }
   
   impl Module for CategoryClassificationHead {
       fn forward(&self, xs: &Tensor) -> Result<Tensor> {
           // Extract [CLS] token (first token)
           let cls_token = xs.i((.., 0))?;
           let xs = candle_nn::ops::dropout(&cls_token, self.dropout, self.training)?;
           self.classifier.forward(&xs)
       }
   }
   
   struct PIIDetectionHead {
       dropout: f64,
       classifier: Linear,
   }
   
   impl PIIDetectionHead {
       fn new(hidden_size: usize, num_pii_types: usize, vb: VarBuilder) -> Result<Self> {
           let classifier = candle_nn::linear(hidden_size, num_pii_types, vb.pp("classifier"))?;
           Ok(Self { dropout: 0.1, classifier })
       }
   }
   
   impl Module for PIIDetectionHead {
       fn forward(&self, xs: &Tensor) -> Result<Tensor> {
           let xs = candle_nn::ops::dropout(xs, self.dropout, self.training)?;
           self.classifier.forward(&xs)
       }
   }
   
   pub struct DualPurposeDistilBERT {
       distilbert: DistilBertModel,
       category_head: CategoryClassificationHead,
       pii_head: PIIDetectionHead,
   }
   
   impl DualPurposeDistilBERT {
       pub fn new(
           config: &DistilBertConfig,
           num_categories: usize,
           num_pii_types: usize,
           vb: VarBuilder,
       ) -> Result<Self> {
           let distilbert = DistilBertModel::new(config, vb.pp("distilbert"))?;
           let category_head = CategoryClassificationHead::new(
               config.hidden_size as usize,
               num_categories,
               vb.pp("category_head"),
           )?;
           let pii_head = PIIDetectionHead::new(
               config.hidden_size as usize,
               num_pii_types,
               vb.pp("pii_head"),
           )?;
           
           Ok(Self {
               distilbert,
               category_head,
               pii_head,
           })
       }
       
       pub fn load_weights(path: &str, device: &Device) -> Result<Self> {
           // Load weights from safetensors file
           // [Implementation details]
       }
   }
   
   impl Module for DualPurposeDistilBERT {
       fn forward(&self, input_ids: &Tensor, attention_mask: Option<&Tensor>) -> Result<(Tensor, Tensor)> {
           let hidden_states = self.distilbert.forward(input_ids, attention_mask)?;
           let category_logits = self.category_head.forward(&hidden_states)?;
           let pii_logits = self.pii_head.forward(&hidden_states)?;
           
           Ok((category_logits, pii_logits))
       }
   }
   ```

2. Implement model loading from Python-trained weights:
   ```rust
   impl DualPurposeDistilBERT {
       pub fn from_pretrained(path: &str, device: &Device) -> Result<Self> {
           // Load safetensors file
           let tensors = unsafe { candle_core::safetensors::MmapedFile::new(path)? };
           let tensors = tensors.deserialize()?;
           
           // Load config
           let config_path = std::path::Path::new(path).with_file_name("config.json");
           let config: DistilBertConfig = serde_json::from_reader(
               std::fs::File::open(config_path)?
           )?;
           
           // Create model with loaded weights
           let vb = VarBuilder::from_tensors(tensors, DType::F32, device);
           Self::new(&config, vb)
       }
   }
   ```

3. Implement tokenization and inference:
   ```rust
   pub struct DualPurposeClassifier {
       model: DualPurposeDistilBERT,
       tokenizer: tokenizers::Tokenizer,
       id2category: HashMap<usize, String>,
       id2pii_type: HashMap<usize, String>,
   }
   
   impl DualPurposeClassifier {
       pub fn new(model_path: &str, tokenizer_path: &str, device: &Device) -> Result<Self> {
           let model = DualPurposeDistilBERT::from_pretrained(model_path, device)?;
           let tokenizer = tokenizers::Tokenizer::from_file(tokenizer_path)?;
           
           // Load category and PII type mappings
           // [Implementation details]
           
           Ok(Self {
               model,
               tokenizer,
               id2category,
               id2pii_type,
           })
       }
       
       pub fn classify(&self, text: &str) -> Result<ClassificationResult> {
           // Tokenize input
           let encoding = self.tokenizer.encode(text, true)?;
           let input_ids = Tensor::new(
               encoding.get_ids().to_vec(),
               &Device::Cpu
           )?.unsqueeze(0)?;
           let attention_mask = Tensor::new(
               encoding.get_attention_mask().to_vec(),
               &Device::Cpu
           )?.unsqueeze(0)?;
           
           // Run inference
           let (category_logits, pii_logits) = self.model.forward(&input_ids, Some(&attention_mask))?;
           
           // Process results
           // [Implementation details]
           
           Ok(ClassificationResult {
               category,
               pii_entities,
           })
       }
   }
   ```

# Test Strategy:
1. Unit tests for Rust model architecture
2. Test loading weights from Python-trained models
3. Compare outputs between Python and Rust implementations to ensure they match
4. Benchmark performance of Rust implementation
5. Test memory usage and ensure it meets requirements
6. Test with various input sizes and edge cases
