# Task ID: 3
# Title: Implement Training Pipeline for Dual-Purpose Model
# Status: done
# Dependencies: 2
# Priority: high
# Description: Create a training pipeline that can train the dual-purpose model on both category classification and PII detection tasks, either jointly or separately, with support for real datasets and hardware capability detection.
# Details:
1. Implement data loading and preprocessing for both tasks with real dataset support:
   ```python
   class DualTaskDataset(Dataset):
       def __init__(self, texts, category_labels, pii_labels, tokenizer, max_length=512):
           self.texts = texts
           self.category_labels = category_labels
           self.pii_labels = pii_labels
           self.tokenizer = tokenizer
           self.max_length = max_length
           
       def __len__(self):
           return len(self.texts)
           
       def __getitem__(self, idx):
           text = self.texts[idx]
           category_label = self.category_labels[idx]
           pii_label = self.pii_labels[idx]
           
           encoding = self.tokenizer(text, truncation=True, padding='max_length', 
                                    max_length=self.max_length, return_tensors='pt')
           
           # Adjust PII labels to match tokenized input
           # [Implementation details for token alignment]
           
           return {
               'input_ids': encoding['input_ids'].squeeze(),
               'attention_mask': encoding['attention_mask'].squeeze(),
               'category_label': torch.tensor(category_label),
               'pii_labels': torch.tensor(adjusted_pii_labels)
           }
   ```

2. Implement dataset loaders for popular datasets:
   ```python
   def load_conll2003(path):
       # Load CoNLL-2003 dataset with proper NER tag conversion to PII labels
       # Handle tokenization alignment for subword tokens
       return texts, category_labels, pii_labels
       
   def load_agnews(path):
       # Load AG News dataset for text classification
       # Generate empty PII labels (if using only for classification)
       return texts, category_labels, pii_labels
       
   def load_wikiner(path):
       # Load WikiNER dataset with proper NER tag conversion
       return texts, category_labels, pii_labels
       
   def detect_and_load_dataset(path):
       # Auto-detect format (JSON, CSV, CoNLL-U) and load appropriately
       # Validate data quality and provide statistics
       if format == 'json':
           return load_json_dataset(path)
       elif format == 'csv':
           return load_csv_dataset(path)
       elif format == 'conll':
           return load_conll_dataset(path)
       else:
           raise ValueError(f"Unsupported format: {format}")
   ```

3. Implement hardware capability detection:
   ```python
   def detect_hardware_capabilities():
       # Detect available hardware (GPU/CPU)
       # Assess available memory
       # Determine optimal batch size and precision
       if torch.cuda.is_available():
           device = 'cuda'
           gpu_mem = torch.cuda.get_device_properties(0).total_memory
           # Calculate appropriate batch size based on model size and GPU memory
           recommended_batch_size = calculate_batch_size(gpu_mem)
           use_mixed_precision = True
       else:
           device = 'cpu'
           recommended_batch_size = 4  # Conservative default for CPU
           use_mixed_precision = False
           
       return {
           'device': device,
           'recommended_batch_size': recommended_batch_size,
           'use_mixed_precision': use_mixed_precision,
           'gradient_accumulation_steps': calculate_grad_accum_steps(recommended_batch_size)
       }
   ```

4. Implement loss function that combines both tasks:
   ```python
   def dual_task_loss(category_logits, category_labels, pii_logits, pii_labels, mask, alpha=0.5):
       category_loss = F.cross_entropy(category_logits, category_labels)
       
       # Only consider tokens that are not padding
       active_loss = mask.view(-1) == 1
       active_logits = pii_logits.view(-1, pii_logits.shape[-1])[active_loss]
       active_labels = pii_labels.view(-1)[active_loss]
       pii_loss = F.cross_entropy(active_logits, active_labels)
       
       # Weighted combination of both losses
       return alpha * category_loss + (1 - alpha) * pii_loss
   ```

5. Implement enhanced training loop with hardware optimization:
   ```python
   def train(model, train_dataloader, optimizer, scheduler, device, config):
       model.train()
       total_loss = 0
       
       # Configure mixed precision if supported
       scaler = torch.cuda.amp.GradScaler() if config['use_mixed_precision'] else None
       
       # Setup checkpointing
       checkpoint_dir = os.path.join(config['output_dir'], 'checkpoints')
       os.makedirs(checkpoint_dir, exist_ok=True)
       
       for batch_idx, batch in enumerate(train_dataloader):
           input_ids = batch['input_ids'].to(device)
           attention_mask = batch['attention_mask'].to(device)
           category_labels = batch['category_label'].to(device)
           pii_labels = batch['pii_labels'].to(device)
           
           # Mixed precision training if available
           if config['use_mixed_precision']:
               with torch.cuda.amp.autocast():
                   outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                   loss = dual_task_loss(
                       outputs['category_logits'], category_labels,
                       outputs['pii_logits'], pii_labels,
                       attention_mask, config['alpha']
                   )
               scaler.scale(loss).backward()
               
               # Gradient accumulation
               if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:
                   scaler.step(optimizer)
                   scaler.update()
                   scheduler.step()
                   optimizer.zero_grad()
           else:
               outputs = model(input_ids=input_ids, attention_mask=attention_mask)
               loss = dual_task_loss(
                   outputs['category_logits'], category_labels,
                   outputs['pii_logits'], pii_labels,
                   attention_mask, config['alpha']
               )
               loss.backward()
               
               # Gradient accumulation
               if (batch_idx + 1) % config['gradient_accumulation_steps'] == 0:
                   optimizer.step()
                   scheduler.step()
                   optimizer.zero_grad()
           
           total_loss += loss.item()
           
           # Save checkpoint periodically
           if (batch_idx + 1) % config['checkpoint_steps'] == 0:
               save_checkpoint(model, optimizer, scheduler, checkpoint_dir, f"checkpoint-{batch_idx}")
       
       return total_loss / len(train_dataloader)
   ```

6. Implement evaluation function for both tasks with detailed metrics:
   ```python
   def evaluate(model, eval_dataloader, device, config):
       model.eval()
       category_preds, category_labels_list = [], []
       pii_preds, pii_labels_list = [], []
       
       with torch.no_grad():
           for batch in eval_dataloader:
               input_ids = batch['input_ids'].to(device)
               attention_mask = batch['attention_mask'].to(device)
               category_labels = batch['category_label'].to(device)
               pii_labels = batch['pii_labels'].to(device)
               
               outputs = model(input_ids=input_ids, attention_mask=attention_mask)
               
               # Process category predictions
               category_preds.extend(outputs['category_logits'].argmax(dim=-1).cpu().numpy())
               category_labels_list.extend(category_labels.cpu().numpy())
               
               # Process PII predictions (handling padding)
               for i in range(len(input_ids)):
                   mask = batch['attention_mask'][i].bool()
                   pred = outputs['pii_logits'][i].argmax(dim=-1)[mask].cpu().numpy()
                   true = pii_labels[i][mask].cpu().numpy()
                   pii_preds.append(pred)
                   pii_labels_list.append(true)
       
       # Calculate metrics
       category_metrics = calculate_classification_metrics(category_preds, category_labels_list)
       pii_metrics = calculate_token_classification_metrics(pii_preds, pii_labels_list)
       
       return {
           'category': category_metrics,
           'pii': pii_metrics,
           'combined_score': (category_metrics['f1'] + pii_metrics['f1']) / 2
       }
   ```

# Test Strategy:
1. Test training pipeline with small synthetic dataset
2. Test with real datasets (CoNLL-2003, AGNews, WikiNER)
3. Verify loss decreases during training
4. Evaluate model performance on validation sets for both tasks
5. Test with different alpha values to balance the tasks
6. Measure memory usage during training to ensure efficiency
7. Test hardware detection on different environments (CPU, single GPU, multi-GPU)
8. Verify automatic batch size selection is appropriate for the hardware
9. Test data loading with various formats (JSON, CSV, CoNLL-U)
10. Verify checkpoint saving and loading functionality
11. Test error handling with malformed datasets
12. Benchmark training speed with and without mixed precision
13. Verify tokenization alignment for PII labels with different tokenizers

# Subtasks:
## 3.1. Implement real dataset loaders [done]
### Dependencies: None
### Description: Create dataset loaders for popular NLP datasets with proper tokenization alignment
### Details:


## 3.2. Implement hardware capability detection [done]
### Dependencies: None
### Description: Create a system to detect available hardware and optimize training parameters accordingly
### Details:


## 3.3. Enhance training loop with hardware optimization [done]
### Dependencies: None
### Description: Update training loop to support mixed precision, gradient accumulation, and checkpointing
### Details:


## 3.4. Implement data format standardization [done]
### Dependencies: None
### Description: Add support for multiple input formats (JSON, CSV, CoNLL-U) with automatic detection
### Details:


## 3.5. Create comprehensive evaluation metrics [done]
### Dependencies: None
### Description: Implement detailed evaluation metrics for both classification and token classification tasks
### Details:


