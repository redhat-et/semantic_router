import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple, Optional, Union
import numpy as np
from tqdm import tqdm
from sklearn.metrics import accuracy_score, classification_report, f1_score
import json
import os
import time
import warnings
from pathlib import Path

# Import our custom modules
from dual_classifier import DualClassifier
from hardware_detector import HardwareDetector, HardwareCapabilities, detect_and_configure, estimate_training_time
from dataset_loaders import RealDatasetLoader, DatasetInfo, load_custom_dataset
from missing_files_detector import check_missing_files

# Interactive selection support
try:
    import inquirer
    INQUIRER_AVAILABLE = True
except ImportError:
    INQUIRER_AVAILABLE = False
    print("üí° For enhanced interactive selection, install: pip install inquirer")


class DualTaskDataset(Dataset):
    """
    Dataset for dual-task learning with category classification and PII detection.
    """
    
    def __init__(
        self,
        texts: List[str],
        category_labels: List[int],
        pii_labels: List[List[int]],  # Token-level PII labels
        tokenizer,
        max_length: int = 512
    ):
        self.texts = texts
        self.category_labels = category_labels
        self.pii_labels = pii_labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        category_label = self.category_labels[idx]
        pii_label = self.pii_labels[idx]
        
        # Tokenize the text
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # Prepare PII labels to match tokenized length
        # Note: This is simplified - in practice you'd need proper token alignment
        pii_labels_padded = pii_label[:self.max_length]
        if len(pii_labels_padded) < self.max_length:
            pii_labels_padded.extend([0] * (self.max_length - len(pii_labels_padded)))
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'category_label': torch.tensor(category_label, dtype=torch.long),
            'pii_labels': torch.tensor(pii_labels_padded, dtype=torch.long)
        }


class DualTaskLoss(nn.Module):
    """
    Combined loss function for dual-task learning.
    """
    
    def __init__(self, category_weight: float = 1.0, pii_weight: float = 1.0):
        super().__init__()
        self.category_weight = category_weight
        self.pii_weight = pii_weight
        self.category_loss_fn = nn.CrossEntropyLoss()
        self.pii_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding tokens
        
    def forward(
        self,
        category_logits: torch.Tensor,
        pii_logits: torch.Tensor,
        category_labels: torch.Tensor,
        pii_labels: torch.Tensor,
        attention_mask: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Calculate combined loss for both tasks.
        
        Returns:
            total_loss, category_loss, pii_loss
        """
        # Category classification loss
        category_loss = self.category_loss_fn(category_logits, category_labels)
        
        # PII detection loss - only compute loss for attended tokens
        # Reshape for loss computation
        pii_logits_flat = pii_logits.view(-1, pii_logits.size(-1))
        pii_labels_flat = pii_labels.view(-1)
        
        # Mask out padded tokens
        attention_mask_flat = attention_mask.view(-1)
        pii_labels_masked = pii_labels_flat.clone()
        pii_labels_masked[attention_mask_flat == 0] = -100
        
        pii_loss = self.pii_loss_fn(pii_logits_flat, pii_labels_masked)
        
        # Combined loss
        total_loss = (self.category_weight * category_loss + 
                     self.pii_weight * pii_loss)
        
        return total_loss, category_loss, pii_loss


class TrainingStrengthConfig:
    """
    Training strength configurations for different training intensities.
    """
    
    CONFIGS = {
        "quick": {
            "description": "Fast training for testing and prototyping",
            "num_epochs": 2,
            "learning_rate_multiplier": 2.0,  # Higher LR for faster convergence
            "batch_size_multiplier": 2.0,     # Larger batches for speed
            "gradient_accumulation_divider": 2, # Less accumulation for speed
            "checkpoint_steps_multiplier": 2.0, # Less frequent checkpoints
            "eval_steps_multiplier": 2.0,     # Less frequent evaluation
            "early_stopping_patience": 3,
            "warmup_ratio": 0.05  # Less warmup
        },
        "normal": {
            "description": "Balanced training for good results in reasonable time",
            "num_epochs": 5,
            "learning_rate_multiplier": 1.0,  # Standard LR
            "batch_size_multiplier": 1.0,     # Standard batch size
            "gradient_accumulation_divider": 1, # Standard accumulation
            "checkpoint_steps_multiplier": 1.0, # Standard checkpointing
            "eval_steps_multiplier": 1.0,     # Standard evaluation
            "early_stopping_patience": 5,
            "warmup_ratio": 0.1
        },
        "intensive": {
            "description": "Thorough training for high-quality results",
            "num_epochs": 10,
            "learning_rate_multiplier": 0.7,  # Lower LR for stability
            "batch_size_multiplier": 0.8,     # Smaller batches for precision
            "gradient_accumulation_divider": 1, # Standard accumulation
            "checkpoint_steps_multiplier": 0.5, # More frequent checkpoints
            "eval_steps_multiplier": 0.5,     # More frequent evaluation
            "early_stopping_patience": 8,
            "warmup_ratio": 0.15  # More warmup
        },
        "maximum": {
            "description": "Maximum quality training - may take hours",
            "num_epochs": 20,
            "learning_rate_multiplier": 0.5,  # Very conservative LR
            "batch_size_multiplier": 0.6,     # Smaller batches
            "gradient_accumulation_divider": 1, # Standard accumulation
            "checkpoint_steps_multiplier": 0.3, # Very frequent checkpoints
            "eval_steps_multiplier": 0.3,     # Very frequent evaluation
            "early_stopping_patience": 12,
            "warmup_ratio": 0.2   # Extensive warmup
        }
    }
    
    @classmethod
    def get_config(cls, strength: str) -> Dict:
        """Get configuration for specified training strength."""
        if strength not in cls.CONFIGS:
            available = list(cls.CONFIGS.keys())
            raise ValueError(f"Training strength '{strength}' not available. Choose from: {available}")
        return cls.CONFIGS[strength].copy()
    
    @classmethod
    def list_strengths(cls) -> Dict[str, str]:
        """List available training strengths with descriptions."""
        return {name: config["description"] for name, config in cls.CONFIGS.items()}


class EnhancedDualTaskTrainer:
    """
    Enhanced trainer for dual-purpose classifier with hardware detection and real dataset support.
    
    Key features:
    - Automatic hardware capability detection
    - Real dataset loading with format detection
    - Mixed precision training support
    - Gradient accumulation for large effective batch sizes
    - Automatic checkpointing and recovery
    - Comprehensive metrics and monitoring
    - Configurable training strength levels
    """
    
    def __init__(
        self,
        model: DualClassifier,
        train_dataset_path: Optional[str] = None,
        val_dataset_path: Optional[str] = None,
        train_dataset: Optional[DualTaskDataset] = None,
        val_dataset: Optional[DualTaskDataset] = None,
        auto_detect_hardware: bool = True,
        training_strength: str = "normal",
        category_weight: float = 1.0,
        pii_weight: float = 1.0,
        output_dir: str = "./training_output",
        **override_config
    ):
        """
        Initialize enhanced trainer.
        
        Args:
            model: DualClassifier model
            train_dataset_path: Path to training dataset file (alternative to train_dataset)
            val_dataset_path: Path to validation dataset file (alternative to val_dataset)
            train_dataset: Pre-loaded training dataset
            val_dataset: Pre-loaded validation dataset
            auto_detect_hardware: Whether to automatically detect and configure hardware
            training_strength: Training intensity level ("quick", "normal", "intensive", "maximum")
            category_weight: Weight for category classification loss
            pii_weight: Weight for PII detection loss
            output_dir: Base directory for outputs (will create subdirectory based on training_strength)
            **override_config: Override hardware-detected configuration
        """
        self.model = model
        
        # Create strength-specific output directory
        base_output_dir = Path(output_dir)
        self.output_dir = base_output_dir / training_strength
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Get training strength configuration
        self.training_strength = training_strength
        self.strength_config = TrainingStrengthConfig.get_config(training_strength)
        
        print(f"üéØ Training Strength: {training_strength}")
        print(f"   {self.strength_config['description']}")
        print(f"   Expected epochs: {self.strength_config['num_epochs']}")
        
        # Hardware detection and configuration
        if auto_detect_hardware:
            print("üîç Detecting hardware capabilities...")
            self.capabilities, self.config = detect_and_configure()
            
            # Apply training strength modifications
            self._apply_strength_config()
            
            # Apply any user overrides
            self.config.update(override_config)
        else:
            # Use default configuration
            self.capabilities = None
            self.config = {
                'device': 'cpu',
                'batch_size': 8,
                'gradient_accumulation_steps': 1,
                'use_mixed_precision': False,
                'num_workers': 0,
                'pin_memory': False,
                'learning_rate': 2e-5,
                'warmup_steps': 100,
                'max_grad_norm': 1.0,
                'checkpoint_steps': 500,
                'eval_steps': 250,
                'fp16': False,
                'bf16': False,
            }
            self._apply_strength_config()
            self.config.update(override_config)
        
        # Set device
        self.device = torch.device(self.config['device'])
        self.model.to(self.device)
        
        # Load datasets
        self.train_dataset, self.val_dataset = self._prepare_datasets(
            train_dataset_path, val_dataset_path, train_dataset, val_dataset
        )
        
        # Setup training components
        self._setup_training_components(category_weight, pii_weight)
        
        # Training state
        self.current_epoch = 0
        self.global_step = 0
        self.best_val_score = 0.0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'val_category_acc': [],
            'val_pii_f1': [],
            'learning_rates': [],
            'epochs': []
        }
        
        print(f"‚úÖ Enhanced trainer initialized")
        print(f"   üìÅ Output directory: {self.output_dir}")
        print(f"   üéØ Training samples: {len(self.train_dataset) if self.train_dataset else 0}")
        print(f"   üéØ Validation samples: {len(self.val_dataset) if self.val_dataset else 0}")
        print(f"   ‚öôÔ∏è Effective batch size: {self.config['batch_size'] * self.config['gradient_accumulation_steps']}")
        print(f"   ‚öôÔ∏è Learning rate: {self.config['learning_rate']:.2e}")
    
    def _apply_strength_config(self):
        """Apply training strength configuration to base config."""
        # Adjust learning rate
        base_lr = self.config.get('learning_rate', 2e-5)
        self.config['learning_rate'] = base_lr * self.strength_config['learning_rate_multiplier']
        
        # Adjust batch size
        base_batch = self.config.get('batch_size', 8)
        new_batch = int(base_batch * self.strength_config['batch_size_multiplier'])
        self.config['batch_size'] = max(1, new_batch)
        
        # Adjust gradient accumulation
        base_accum = self.config.get('gradient_accumulation_steps', 1)
        new_accum = max(1, base_accum // self.strength_config['gradient_accumulation_divider'])
        self.config['gradient_accumulation_steps'] = new_accum
        
        # Adjust checkpoint frequency
        base_checkpoint = self.config.get('checkpoint_steps', 500)
        new_checkpoint = int(base_checkpoint * self.strength_config['checkpoint_steps_multiplier'])
        self.config['checkpoint_steps'] = max(50, new_checkpoint)
        
        # Adjust evaluation frequency
        base_eval = self.config.get('eval_steps', 250)
        new_eval = int(base_eval * self.strength_config['eval_steps_multiplier'])
        self.config['eval_steps'] = max(25, new_eval)
        
        # Adjust warmup
        base_warmup = self.config.get('warmup_steps', 100)
        warmup_ratio = self.strength_config['warmup_ratio']
        # We'll calculate actual warmup steps later when we know dataset size
        self.config['warmup_ratio'] = warmup_ratio
        
        # Store early stopping patience
        self.config['early_stopping_patience'] = self.strength_config['early_stopping_patience']
        
        print(f"   ‚öôÔ∏è Strength adjustments applied:")
        print(f"      Learning rate: {self.config['learning_rate']:.2e}")
        print(f"      Batch size: {self.config['batch_size']}")
        print(f"      Gradient accumulation: {self.config['gradient_accumulation_steps']}")
        print(f"      Checkpoint every: {self.config['checkpoint_steps']} steps")
        print(f"      Evaluate every: {self.config['eval_steps']} steps")
    
    def _prepare_datasets(
        self,
        train_path: Optional[str],
        val_path: Optional[str],
        train_dataset: Optional[DualTaskDataset],
        val_dataset: Optional[DualTaskDataset]
    ) -> Tuple[Optional[DualTaskDataset], Optional[DualTaskDataset]]:
        """Prepare training and validation datasets with improved category handling."""
        
        # Use provided datasets if available
        if train_dataset is not None:
            return train_dataset, val_dataset
        
        # Check for missing files if no paths provided
        if not train_path and not val_path:
            print("‚ö†Ô∏è  No dataset paths provided. Checking for available dataset files...")
            if not check_missing_files("training"):
                print("\nüí° Run one of the dataset generators first, then retry training.")
                raise FileNotFoundError("Required dataset files not found. See instructions above.")
            
            # Auto-detect dataset files if they exist
            if os.path.exists("real_train_dataset.json"):
                train_path = "real_train_dataset.json"
                print(f"‚úÖ Auto-detected training dataset: {train_path}")
            elif os.path.exists("datasets/real_train_dataset.json"):
                train_path = "datasets/real_train_dataset.json"
                print(f"‚úÖ Auto-detected training dataset: {train_path}")
            
            if os.path.exists("real_val_dataset.json"):
                val_path = "real_val_dataset.json"
                print(f"‚úÖ Auto-detected validation dataset: {val_path}")
            elif os.path.exists("datasets/real_val_dataset.json"):
                val_path = "datasets/real_val_dataset.json"
                print(f"‚úÖ Auto-detected validation dataset: {val_path}")
        
        # Analyze categories and create mappings
        if train_path and val_path:
            # Get category mappings
            category_to_id, id_to_category, num_categories = analyze_dataset_categories(train_path, val_path)
            
            # Store category mappings for later use
            self.category_to_id = category_to_id
            self.id_to_category = id_to_category
            self.num_categories = num_categories
            
            # Verify model has correct number of categories
            model_categories = self.model.category_classifier[-1].out_features
            if model_categories != num_categories:
                print(f"‚ùå Model architecture mismatch!")
                print(f"   Model expects: {model_categories} categories")
                print(f"   Dataset has: {num_categories} categories")
                raise ValueError(f"Model architecture mismatch. Initialize model with num_categories={num_categories}")
            
            print(f"‚úÖ Model architecture matches dataset ({num_categories} categories)")
            
            # Load training dataset
            print(f"üìä Loading training dataset from: {train_path}")
            train_texts, train_categories, train_pii_labels = create_enhanced_dataset(
                train_path, category_to_id, self.model.tokenizer, self.model.max_length
            )
            
            train_dataset = DualTaskDataset(
                train_texts, train_categories, train_pii_labels,
                self.model.tokenizer, max_length=self.model.max_length
            )
            
            # Load validation dataset
            print(f"üìä Loading validation dataset from: {val_path}")
            val_texts, val_categories, val_pii_labels = create_enhanced_dataset(
                val_path, category_to_id, self.model.tokenizer, self.model.max_length
            )
            
            val_dataset = DualTaskDataset(
                val_texts, val_categories, val_pii_labels,
                self.model.tokenizer, max_length=self.model.max_length
            )
            
            # Estimate training time
            if self.capabilities:
                estimated_time = estimate_training_time(
                    len(train_texts), self.capabilities, num_epochs=3
                )
                print(f"‚è±Ô∏è  Estimated training time: {estimated_time}")
            
        else:
            train_dataset = None
            val_dataset = None
        
        return train_dataset, val_dataset
    
    def _setup_training_components(self, category_weight: float, pii_weight: float):
        """Setup loss function, optimizer, scheduler, and data loaders."""
        
        # Loss function
        self.loss_fn = DualTaskLoss(category_weight, pii_weight)
        
        # Optimizer
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=0.01
        )
        
        # Data loaders
        if self.train_dataset:
            self.train_loader = DataLoader(
                self.train_dataset,
                batch_size=self.config['batch_size'],
                shuffle=True,
                num_workers=self.config['num_workers'],
                pin_memory=self.config['pin_memory'],
                drop_last=self.config.get('dataloader_drop_last', False)
            )
        else:
            self.train_loader = None
        
        if self.val_dataset:
            self.val_loader = DataLoader(
                self.val_dataset,
                batch_size=self.config['batch_size'],
                shuffle=False,
                num_workers=self.config['num_workers'],
                pin_memory=self.config['pin_memory']
            )
        else:
            self.val_loader = None
        
        # Scheduler (setup after knowing number of steps)
        if self.train_loader:
            total_steps = len(self.train_loader) * 3  # Assume 3 epochs for now
            self.scheduler = torch.optim.lr_scheduler.LinearLR(
                self.optimizer,
                start_factor=0.1,
                total_iters=self.config['warmup_steps']
            )
        else:
            self.scheduler = None
        
        # Mixed precision scaler
        if self.config['use_mixed_precision'] and self.device.type == 'cuda':
            self.scaler = torch.cuda.amp.GradScaler()
        else:
            self.scaler = None
    
    def train_epoch(self) -> Dict[str, float]:
        """Train for one epoch with advanced features."""
        if not self.train_loader:
            raise ValueError("No training dataset provided")
        
        self.model.train()
        total_loss = 0
        total_category_loss = 0
        total_pii_loss = 0
        num_batches = 0
        
        # Setup progress bar
        progress_bar = tqdm(
            self.train_loader,
            desc=f"Epoch {self.current_epoch + 1}",
            leave=False
        )
        
        for batch_idx, batch in enumerate(progress_bar):
            # Move batch to device
            input_ids = batch['input_ids'].to(self.device, non_blocking=True)
            attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
            category_labels = batch['category_label'].to(self.device, non_blocking=True)
            pii_labels = batch['pii_labels'].to(self.device, non_blocking=True)
            
            # Forward pass with mixed precision
            if self.scaler:
                with torch.cuda.amp.autocast():
                    category_logits, pii_logits = self.model(input_ids, attention_mask)
                    loss, cat_loss, pii_loss = self.loss_fn(
                        category_logits, pii_logits, category_labels, pii_labels, attention_mask
                    )
                    # Scale loss for gradient accumulation
                    loss = loss / self.config['gradient_accumulation_steps']
                
                # Backward pass
                self.scaler.scale(loss).backward()
            else:
                category_logits, pii_logits = self.model(input_ids, attention_mask)
                loss, cat_loss, pii_loss = self.loss_fn(
                    category_logits, pii_logits, category_labels, pii_labels, attention_mask
                )
                # Scale loss for gradient accumulation
                loss = loss / self.config['gradient_accumulation_steps']
                
                # Backward pass
                loss.backward()
            
            # Update metrics
            total_loss += loss.item()
            total_category_loss += cat_loss.item()
            total_pii_loss += pii_loss.item()
            num_batches += 1
            
            # Gradient accumulation
            if (batch_idx + 1) % self.config['gradient_accumulation_steps'] == 0:
                # Gradient clipping
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config['max_grad_norm'])
                    self.optimizer.step()
                
                if self.scheduler:
                    self.scheduler.step()
                
                self.optimizer.zero_grad()
                self.global_step += 1
                
                # Checkpointing
                if self.global_step % self.config['checkpoint_steps'] == 0:
                    self._save_checkpoint(f"checkpoint-step-{self.global_step}")
            
            # Update progress bar
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'cat_loss': f'{cat_loss.item():.4f}',
                'pii_loss': f'{pii_loss.item():.4f}',
                'lr': f'{self.scheduler.get_last_lr()[0]:.2e}' if self.scheduler else f'{self.config["learning_rate"]:.2e}'
            })
        
        # Calculate averages
        avg_loss = total_loss / num_batches
        avg_cat_loss = total_category_loss / num_batches
        avg_pii_loss = total_pii_loss / num_batches
        
        return {
            'train_loss': avg_loss,
            'train_category_loss': avg_cat_loss,
            'train_pii_loss': avg_pii_loss
        }
    
    def evaluate(self) -> Dict[str, float]:
        """Evaluate on validation set with detailed metrics."""
        if not self.val_loader:
            return {}
        
        self.model.eval()
        total_loss = 0
        total_category_loss = 0
        total_pii_loss = 0
        num_batches = 0
        
        all_category_preds = []
        all_category_labels = []
        all_pii_preds = []
        all_pii_labels = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Evaluating", leave=False):
                # Move batch to device
                input_ids = batch['input_ids'].to(self.device, non_blocking=True)
                attention_mask = batch['attention_mask'].to(self.device, non_blocking=True)
                category_labels = batch['category_label'].to(self.device, non_blocking=True)
                pii_labels = batch['pii_labels'].to(self.device, non_blocking=True)
                
                # Forward pass
                if self.scaler:
                    with torch.cuda.amp.autocast():
                        category_logits, pii_logits = self.model(input_ids, attention_mask)
                        loss, cat_loss, pii_loss = self.loss_fn(
                            category_logits, pii_logits, category_labels, pii_labels, attention_mask
                        )
                else:
                    category_logits, pii_logits = self.model(input_ids, attention_mask)
                    loss, cat_loss, pii_loss = self.loss_fn(
                        category_logits, pii_logits, category_labels, pii_labels, attention_mask
                    )
                
                # Update loss metrics
                total_loss += loss.item()
                total_category_loss += cat_loss.item()
                total_pii_loss += pii_loss.item()
                num_batches += 1
                
                # Collect predictions for metrics
                category_preds = torch.argmax(category_logits, dim=1)
                all_category_preds.extend(category_preds.cpu().numpy())
                all_category_labels.extend(category_labels.cpu().numpy())
                
                # PII predictions (only for non-padded tokens)
                pii_preds = torch.argmax(pii_logits, dim=2)
                for i in range(len(input_ids)):
                    mask = attention_mask[i].cpu().numpy()
                    valid_length = mask.sum()
                    all_pii_preds.extend(pii_preds[i][:valid_length].cpu().numpy())
                    all_pii_labels.extend(pii_labels[i][:valid_length].cpu().numpy())
        
        # Calculate metrics
        avg_loss = total_loss / num_batches
        avg_cat_loss = total_category_loss / num_batches
        avg_pii_loss = total_pii_loss / num_batches
        
        category_acc = accuracy_score(all_category_labels, all_category_preds)
        pii_f1 = f1_score(all_pii_labels, all_pii_preds, average='weighted', zero_division=0)
        
        # Combined score for model selection
        combined_score = (category_acc + pii_f1) / 2
        
        return {
            'val_loss': avg_loss,
            'val_category_loss': avg_cat_loss,
            'val_pii_loss': avg_pii_loss,
            'val_category_acc': category_acc,
            'val_pii_f1': pii_f1,
            'val_combined_score': combined_score
        }
    
    def train(self, num_epochs: Optional[int] = None, save_best_model: bool = True):
        """
        Train the model for specified epochs.
        
        Args:
            num_epochs: Number of epochs (uses training strength default if None)
            save_best_model: Whether to save the best model during training
        """
        # Use training strength default if not specified
        if num_epochs is None:
            num_epochs = self.strength_config['num_epochs']
        
        print(f"\nüöÄ Starting enhanced training for {num_epochs} epochs")
        print(f"   üìä Training samples: {len(self.train_dataset)}")
        if self.val_dataset:
            print(f"   üìä Validation samples: {len(self.val_dataset)}")
        print(f"   ‚öôÔ∏è Device: {self.device}")
        print(f"   ‚öôÔ∏è Mixed precision: {self.config['use_mixed_precision']}")
        print(f"   ‚öôÔ∏è Batch size: {self.config['batch_size']}")
        print(f"   ‚öôÔ∏è Gradient accumulation: {self.config['gradient_accumulation_steps']}")
        print(f"   ‚öôÔ∏è Training strength: {self.training_strength}")
        
        # Estimate training time
        if self.capabilities and self.train_dataset:
            estimated_time = estimate_training_time(
                len(self.train_dataset), self.capabilities, num_epochs=num_epochs
            )
            print(f"   ‚è±Ô∏è  Estimated training time: {estimated_time}")
        
        start_time = time.time()
        early_stopping_counter = 0
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            
            print(f"\nüìà Epoch {epoch + 1}/{num_epochs}")
            
            # Training
            train_metrics = self.train_epoch()
            
            # Evaluation
            if self.val_dataset:
                val_metrics = self.evaluate()
                
                # Early stopping check
                if val_metrics['val_combined_score'] > self.best_val_score:
                    self.best_val_score = val_metrics['val_combined_score']
                    early_stopping_counter = 0
                    
                    if save_best_model:
                        self._save_checkpoint("best_model")
                        print(f"‚úÖ New best model saved! Combined score: {self.best_val_score:.4f}")
                else:
                    early_stopping_counter += 1
                
                # Check early stopping
                patience = self.config['early_stopping_patience']
                if early_stopping_counter >= patience:
                    print(f"üõë Early stopping triggered after {patience} epochs without improvement")
                    break
                
                # Log metrics
                print(f"   üìä Train Loss: {train_metrics['train_loss']:.4f}")
                print(f"   üìä Val Loss: {val_metrics['val_loss']:.4f}")
                print(f"   üìä Category Acc: {val_metrics['val_category_acc']:.4f}")
                print(f"   üìä PII F1: {val_metrics['val_pii_f1']:.4f}")
                print(f"   üìä Combined Score: {val_metrics['val_combined_score']:.4f}")
                print(f"   üìä Early stopping: {early_stopping_counter}/{patience}")
                
                # Update history
                self.training_history['val_loss'].append(val_metrics['val_loss'])
                self.training_history['val_category_acc'].append(val_metrics['val_category_acc'])
                self.training_history['val_pii_f1'].append(val_metrics['val_pii_f1'])
            else:
                print(f"   üìä Train Loss: {train_metrics['train_loss']:.4f}")
            
            # Update history
            self.training_history['train_loss'].append(train_metrics['train_loss'])
            self.training_history['epochs'].append(epoch + 1)
            if self.scheduler:
                self.training_history['learning_rates'].append(self.scheduler.get_last_lr()[0])
            
            # Save epoch checkpoint
            self._save_checkpoint(f"epoch-{epoch + 1}")
        
        total_time = time.time() - start_time
        print(f"\nüéâ Training completed in {total_time:.1f} seconds")
        
        # Save final model and history
        self._save_final_model()
        self._save_training_history()
        
        return self.training_history
    
    def _save_checkpoint(self, checkpoint_name: str):
        """Save model checkpoint."""
        checkpoint_dir = self.output_dir / "checkpoints"
        checkpoint_dir.mkdir(exist_ok=True)
        
        checkpoint_path = checkpoint_dir / f"{checkpoint_name}.pt"
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'scaler_state_dict': self.scaler.state_dict() if self.scaler else None,
            'epoch': self.current_epoch,
            'global_step': self.global_step,
            'best_val_score': self.best_val_score,
            'config': self.config,
            'training_history': self.training_history
        }, checkpoint_path)
    
    def _save_final_model(self):
        """Save the final trained model with category mappings."""
        final_model_dir = self.output_dir / "final_model"
        final_model_dir.mkdir(exist_ok=True)
        
        # Save model using the DualClassifier's method
        self.model.save_pretrained(str(final_model_dir))
        
        # Save training configuration
        config_path = final_model_dir / "training_config.json"
        config_to_save = self.config.copy()
        
        # Add category mappings if available
        if hasattr(self, 'category_to_id') and hasattr(self, 'id_to_category'):
            config_to_save['categories'] = {
                'category_to_id': self.category_to_id,
                'id_to_category': self.id_to_category,
                'num_categories': self.num_categories
            }
            print(f"üíæ Saved category mappings ({self.num_categories} categories)")
        
        with open(config_path, 'w') as f:
            json.dump(config_to_save, f, indent=2, default=str)
    
    def _save_training_history(self):
        """Save training history."""
        history_path = self.output_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
    
    def load_checkpoint(self, checkpoint_path: str):
        """Load model from checkpoint."""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        if self.scheduler and checkpoint['scheduler_state_dict']:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        if self.scaler and checkpoint['scaler_state_dict']:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        self.current_epoch = checkpoint['epoch']
        self.global_step = checkpoint['global_step']
        self.best_val_score = checkpoint['best_val_score']
        self.training_history = checkpoint['training_history']
        
        print(f"‚úÖ Loaded checkpoint from epoch {self.current_epoch}")


def analyze_dataset_categories(train_path: str, val_path: str) -> Tuple[Dict[str, int], Dict[int, str], int]:
    """
    Comprehensively analyze dataset categories and create proper mappings.
    
    Returns:
        category_to_id: Dict mapping category names to IDs
        id_to_category: Dict mapping IDs to category names  
        num_categories: Total number of categories
    """
    print("üîç Analyzing dataset categories...")
    
    # Collect all categories from both training and validation sets
    all_categories = set()
    
    # Analyze training set
    try:
        with open(train_path, 'r') as f:
            train_data = json.load(f)
        
        train_categories = set()
        for item in train_data:  # Check ALL samples, not just first 100
            if 'category' in item:
                category = item['category']
                all_categories.add(category)
                train_categories.add(category)
        
        print(f"üìä Training set: {len(train_data)} samples, {len(train_categories)} categories")
        print(f"   Categories: {sorted(train_categories)}")
        
    except Exception as e:
        print(f"‚ùå Error reading training set: {e}")
        raise
    
    # Analyze validation set
    try:
        with open(val_path, 'r') as f:
            val_data = json.load(f)
        
        val_categories = set()
        for item in val_data:  # Check ALL samples
            if 'category' in item:
                category = item['category']
                all_categories.add(category)
                val_categories.add(category)
        
        print(f"üìä Validation set: {len(val_data)} samples, {len(val_categories)} categories")
        print(f"   Categories: {sorted(val_categories)}")
        
    except Exception as e:
        print(f"‚ùå Error reading validation set: {e}")
        raise
    
    # Check for category mismatches
    train_only = train_categories - val_categories
    val_only = val_categories - train_categories
    
    if train_only:
        print(f"‚ö†Ô∏è Categories only in training: {sorted(train_only)}")
    if val_only:
        print(f"‚ö†Ô∏è Categories only in validation: {sorted(val_only)}")
    
    # Create consistent mapping
    sorted_categories = sorted(all_categories)
    num_categories = len(sorted_categories)
    
    # Create bidirectional mappings
    category_to_id = {category: idx for idx, category in enumerate(sorted_categories)}
    id_to_category = {idx: category for idx, category in enumerate(sorted_categories)}
    
    print(f"\n‚úÖ Final category mapping ({num_categories} categories):")
    for idx, category in id_to_category.items():
        print(f"   {idx}: {category}")
    
    return category_to_id, id_to_category, num_categories


def create_enhanced_dataset(data_path: str, category_to_id: Dict[str, int], tokenizer, max_length: int = 512) -> Tuple[List[str], List[int], List[List[int]]]:
    """
    Create enhanced dataset with proper category mapping and PII detection.
    
    Returns:
        texts: List of text samples
        category_labels: List of category IDs (mapped consistently)
        pii_labels: List of PII labels for each sample
    """
    print(f"üì¶ Processing dataset: {data_path}")
    
    with open(data_path, 'r') as f:
        data = json.load(f)
    
    texts = []
    category_labels = []
    pii_labels = []
    
    # Simple PII detection patterns
    import re
    pii_patterns = {
        'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        'phone': r'\b(?:\+?1[-.\s]?)?\(?[0-9]{3}\)?[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}\b',
        'ssn': r'\b\d{3}[-.]?\d{2}[-.]?\d{4}\b',
        'credit_card': r'\b(?:\d{4}[-\s]?){3}\d{4}\b',
    }
    
    for item in data:
        if 'text' not in item or 'category' not in item:
            continue
        
        text = item['text']
        category = item['category']
        
        # Map category to ID
        if category not in category_to_id:
            print(f"‚ö†Ô∏è Unknown category '{category}' - skipping sample")
            continue
        
        category_id = category_to_id[category]
        
        # Simple PII detection (word-level)
        words = text.split()
        word_pii_labels = [0] * len(words)
        
        # Check for PII patterns
        for pii_type, pattern in pii_patterns.items():
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                # Mark words that overlap with PII
                start_char, end_char = match.span()
                char_pos = 0
                for i, word in enumerate(words):
                    word_start = char_pos
                    word_end = char_pos + len(word)
                    if word_start < end_char and word_end > start_char:
                        word_pii_labels[i] = 1
                    char_pos = word_end + 1  # +1 for space
        
        texts.append(text)
        category_labels.append(category_id)
        pii_labels.append(word_pii_labels)
    
    print(f"‚úÖ Processed {len(texts)} samples")
    
    # Show category distribution
    category_counts = {}
    for cat_id in category_labels:
        category_counts[cat_id] = category_counts.get(cat_id, 0) + 1
    
    print("üìä Category distribution:")
    for cat_id, count in sorted(category_counts.items()):
        category_name = next(name for name, id in category_to_id.items() if id == cat_id)
        print(f"   {cat_id} ({category_name}): {count} samples")
    
    return texts, category_labels, pii_labels


def detect_available_datasets():
    """
    Detect available datasets in the current directory and datasets/ subdirectory.
    Returns a list of dataset info dictionaries with comprehensive category analysis.
    """
    datasets = []
    
    # Standard dataset files to look for
    dataset_files = [
        ("real_train_dataset.json", "real_val_dataset.json", "Custom Dataset", "Your custom dataset files"),
        ("extended_train_dataset.json", "extended_val_dataset.json", "Extended Dataset", "Generated multi-category dataset"),
    ]
    
    # Look in both current directory and datasets/ subdirectory
    search_paths = ["./", "./datasets/"]
    
    for search_path in search_paths:
        for train_file, val_file, name, description in dataset_files:
            train_path = os.path.join(search_path, train_file)
            val_path = os.path.join(search_path, val_file)
            
            if os.path.exists(train_path) and os.path.exists(val_path):
                # Use our improved category analysis
                try:
                    category_to_id, id_to_category, num_categories = analyze_dataset_categories(train_path, val_path)
                    
                    # Get sample count
                    with open(train_path, 'r') as f:
                        train_data = json.load(f)
                    
                    datasets.append({
                        'name': name,
                        'description': f"{description} ({num_categories} categories)",
                        'train_path': train_path,
                        'val_path': val_path,
                        'categories': num_categories,
                        'category_list': [id_to_category[i] for i in sorted(id_to_category.keys())],
                        'samples': len(train_data),
                        'category_to_id': category_to_id,
                        'id_to_category': id_to_category
                    })
                except Exception as e:
                    print(f"‚ö†Ô∏è Error analyzing {train_path}: {e}")
    
    return datasets

def interactive_dataset_selection():
    """
    Interactive dataset selection with arrow key navigation.
    """
    print("üîç Detecting available datasets...")
    datasets = detect_available_datasets()
    
    if not datasets:
        print("\n‚ùå No datasets found!")
        print("üí° Available dataset downloaders:")
        print("   üì∞ python datasets/generators/download_bbc_dataset.py      (5 categories: business, entertainment, politics, sport, tech)")
        print("   üì∞ python datasets/generators/download_20newsgroups.py     (20 categories: various topics)")
        print("   üì∞ python datasets/generators/download_agnews.py           (4 categories: world, sports, business, technology)")
        print("   üõ†Ô∏è  python datasets/generators/create_multi_category_dataset.py  (8 categories: custom generated)")
        print("\nüöÄ Run one of these scripts first, then come back!")
        return None, None
    
    print(f"\nüìä Found {len(datasets)} available dataset(s):")
    
    if INQUIRER_AVAILABLE:
        # Create choices for inquirer
        choices = []
        for i, dataset in enumerate(datasets):
            choice_text = f"{dataset['name']} - {dataset['categories']} categories, {dataset['samples']} samples"
            choices.append((choice_text, i))
        
        questions = [
            inquirer.List('dataset',
                message="üìÇ Select dataset",
                choices=choices,
                default=choices[0][1] if choices else None,
            ),
        ]
        
        try:
            answers = inquirer.prompt(questions)
            if answers and 'dataset' in answers:
                selected_idx = answers['dataset']
                selected_dataset = datasets[selected_idx]
                print(f"\n‚úÖ Selected: {selected_dataset['name']}")
                print(f"   üìä Categories ({selected_dataset['categories']}): {', '.join(selected_dataset['category_list'])}")
                return selected_dataset['train_path'], selected_dataset['val_path']
        except KeyboardInterrupt:
            print("\nüëã Selection cancelled.")
            return None, None
    else:
        # Fallback to numbered selection
        for i, dataset in enumerate(datasets):
            print(f"   {i+1}. {dataset['name']}")
            print(f"      üìä Categories: {dataset['categories']} ({', '.join(dataset['category_list'])})")
            print(f"      üìÑ Samples: {dataset['samples']}")
            print(f"      üìÅ Files: {dataset['train_path']}, {dataset['val_path']}")
            print()
        
        while True:
            try:
                choice = input(f"Select dataset (1-{len(datasets)}) [1]: ").strip()
                if not choice:
                    choice = "1"
                
                idx = int(choice) - 1
                if 0 <= idx < len(datasets):
                    selected_dataset = datasets[idx]
                    print(f"\n‚úÖ Selected: {selected_dataset['name']}")
                    return selected_dataset['train_path'], selected_dataset['val_path']
                else:
                    print(f"‚ùå Please enter a number between 1 and {len(datasets)}")
            except ValueError:
                print("‚ùå Please enter a valid number")
            except KeyboardInterrupt:
                print("\nüëã Selection cancelled.")
                return None, None
    
    return None, None

def interactive_strength_selection():
    """
    Interactive training strength selection with arrow key navigation.
    """
    strengths = TrainingStrengthConfig.list_strengths()
    
    print("\nüéØ Available Training Strengths:")
    for strength, description in strengths.items():
        epochs = TrainingStrengthConfig.get_config(strength)['num_epochs']
        print(f"   {strength.upper()}: {description} ({epochs} epochs)")
    
    if INQUIRER_AVAILABLE:
        # Create choices for inquirer
        choices = []
        for strength, description in strengths.items():
            epochs = TrainingStrengthConfig.get_config(strength)['num_epochs']
            choice_text = f"{strength.upper()} - {description} ({epochs} epochs)"
            choices.append((choice_text, strength))
        
        questions = [
            inquirer.List('strength',
                message="‚ö° Select training strength",
                choices=choices,
                default='normal',
            ),
        ]
        
        try:
            answers = inquirer.prompt(questions)
            if answers and 'strength' in answers:
                selected_strength = answers['strength']
                print(f"\n‚úÖ Selected training strength: {selected_strength.upper()}")
                return selected_strength
        except KeyboardInterrupt:
            print("\nüëã Selection cancelled.")
            return None
    else:
        # Fallback to numbered selection
        strength_list = list(strengths.keys())
        for i, strength in enumerate(strength_list):
            epochs = TrainingStrengthConfig.get_config(strength)['num_epochs']
            marker = " (default)" if strength == 'normal' else ""
            print(f"   {i+1}. {strength.upper()}: {strengths[strength]} ({epochs} epochs){marker}")
        
        while True:
            try:
                choice = input(f"Select strength (1-{len(strength_list)}) [2 for normal]: ").strip()
                if not choice:
                    choice = "2"  # Default to normal
                
                idx = int(choice) - 1
                if 0 <= idx < len(strength_list):
                    selected_strength = strength_list[idx]
                    print(f"\n‚úÖ Selected training strength: {selected_strength.upper()}")
                    return selected_strength
                else:
                    print(f"‚ùå Please enter a number between 1 and {len(strength_list)}")
            except ValueError:
                print("‚ùå Please enter a valid number")
            except KeyboardInterrupt:
                print("\nüëã Selection cancelled.")
                return None
    
    return 'normal'  # Default fallback


def create_sample_real_dataset(output_path: str, num_samples: int = 100):
    """
    Create a sample real dataset in JSON format for testing.
    
    This simulates what a real dataset might look like.
    """
    import random
    
    categories = ['technology', 'science', 'politics', 'sports', 'business']
    
    samples = []
    for i in range(num_samples):
        category = random.choice(categories)
        
        # Create sample texts with potential PII
        base_texts = {
            'technology': [
                "How does artificial intelligence work?",
                "The latest smartphone features are impressive",
                "Cloud computing is transforming businesses",
                "Cybersecurity threats are increasing",
                "Machine learning algorithms are complex"
            ],
            'science': [
                "Climate change affects global temperatures",
                "DNA research reveals new insights",
                "Space exploration continues to advance",
                "Medical breakthroughs save lives",
                "Quantum physics explains reality"
            ],
            'politics': [
                "Election results vary by region",
                "Policy changes affect citizens",
                "Government spending increases annually",
                "International relations remain complex",
                "Political debates shape public opinion"
            ],
            'sports': [
                "The championship game was exciting",
                "Athletes train for months",
                "Team performance exceeded expectations",
                "Sports statistics reveal trends",
                "Coaching strategies influence outcomes"
            ],
            'business': [
                "Market trends affect stock prices",
                "Company profits increased quarterly",
                "Economic indicators show growth",
                "Investment strategies vary widely",
                "Business partnerships drive success"
            ]
        }
        
        text = random.choice(base_texts[category])
        
        # Occasionally add PII for testing
        if random.random() < 0.3:
            pii_additions = [
                " Contact John Smith at john.smith@company.com",
                " Call 555-123-4567 for more information",
                " Visit our office at 123 Main Street, New York",
                " Email support@business.com for help",
                " Reach out to Sarah Johnson for details"
            ]
            text += random.choice(pii_additions)
        
        sample = {
            'text': text,
            'category': category
        }
        
        samples.append(sample)
    
    # Save to JSON
    with open(output_path, 'w') as f:
        json.dump(samples, f, indent=2)
    
    print(f"‚úÖ Created sample dataset with {num_samples} samples at {output_path}")


if __name__ == "__main__":
    import sys
    import argparse
    
    # Enhanced trainer with real data and interactive selection
    print("üöÄ Enhanced Dual-Purpose Classifier Training")
    print("=" * 60)
    
    # Interactive dataset selection
    train_path, val_path = interactive_dataset_selection()
    if not train_path or not val_path:
        print("üëã Exiting...")
        sys.exit(0)
    
    # Interactive strength selection
    training_strength = interactive_strength_selection()
    if not training_strength:
        print("üëã Exiting...")
        sys.exit(0)
    
    # Analyze dataset categories comprehensively
    try:
        category_to_id, id_to_category, num_categories = analyze_dataset_categories(train_path, val_path)
        
        print(f"\nüìÇ Dataset Analysis Complete:")
        print(f"   üìä Total categories: {num_categories}")
        print(f"   üìã Category mapping: {dict(list(id_to_category.items())[:5])}{'...' if num_categories > 5 else ''}")
        
    except Exception as e:
        print(f"‚ùå Error analyzing dataset categories: {e}")
        print("‚ö†Ô∏è This might indicate dataset format issues.")
        sys.exit(1)
    
    # Initialize model with correct number of categories
    print(f"\nüß† Initializing model with {num_categories} categories...")
    model = DualClassifier(num_categories=num_categories)
    
    # Base output directory (trainer will add strength subdirectory)
    output_dir = "./training_output"
    
    # Create enhanced trainer with selected parameters
    print(f"üèãÔ∏è  Setting up enhanced trainer...")
    try:
        trainer = EnhancedDualTaskTrainer(
            model=model,
            train_dataset_path=train_path,
            val_dataset_path=val_path,
            auto_detect_hardware=True,
            training_strength=training_strength,
            output_dir=output_dir
        )
        
        # Show training summary
        strength_config = TrainingStrengthConfig.get_config(training_strength)
        expected_epochs = strength_config['num_epochs']
        
        print(f"\nüöÄ Training Configuration:")
        print(f"   üìÇ Dataset: {train_path}")
        print(f"   üéØ Training strength: {training_strength.upper()}")
        print(f"   üìä Categories: {num_categories}")
        print(f"   üîÑ Expected epochs: {expected_epochs}")
        print(f"   üìÅ Output directory: {output_dir}/{training_strength}")
        print(f"   ‚è±Ô∏è  Early stopping patience: {strength_config['early_stopping_patience']}")
        
        # Final confirmation
        confirm = input(f"\nüî• Start training with these settings? (Y/n): ").strip().lower()
        if confirm and confirm not in ['y', 'yes', '']:
            print("üëã Training cancelled.")
            sys.exit(0)
        
        # Start training
        print(f"\nüî• Starting {training_strength} training...")
        history = trainer.train(save_best_model=True)
        
        # Show final results
        if history['val_category_acc']:
            final_acc = history['val_category_acc'][-1]
            final_f1 = history['val_pii_f1'][-1]
            print(f"\nüéâ Training completed successfully!")
            print(f"üìä Final Results:")
            print(f"   Category Accuracy: {final_acc:.3f}")
            print(f"   PII F1 Score: {final_f1:.3f}")
            print(f"   Model saved to: {output_dir}/{training_strength}/final_model/")
            
            # Show improvement
            if len(history['val_category_acc']) > 1:
                initial_acc = history['val_category_acc'][0]
                improvement = final_acc - initial_acc
                print(f"   Improvement: +{improvement:.3f} accuracy")
        else:
            print("‚úÖ Training completed (no validation metrics)")
            
    except KeyboardInterrupt:
        print("\n‚õî Training interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()
        
        # Show helpful tips
        print(f"\nüí° Training Tips:")
        print("   - Try 'quick' strength for faster testing")
        print("   - Ensure you have enough memory for the batch size")
        print("   - Check that your datasets are in the correct format")
        print("   - GPU training is much faster if available")
        print("   - Install inquirer for better selection: pip install inquirer")
    
    print(f"\nüìÅ Output saved to: {output_dir}/{training_strength}/")
    print("üöÄ Use your trained model in live_demo.py for testing!") 